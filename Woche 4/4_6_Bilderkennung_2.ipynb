{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianwarmuth/openhpi-kipraxis/blob/main/Woche%204/4_6_Bilderkennung_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb899884",
      "metadata": {
        "id": "eb899884"
      },
      "source": [
        "## 0. Installieren aller Pakete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KdQWt3vYbvgw",
      "metadata": {
        "id": "KdQWt3vYbvgw"
      },
      "outputs": [],
      "source": [
        "# Hier die Kaggle Credentials einfügen (ohne Anführungszeichen)\n",
        "\n",
        "%env KAGGLE_USERNAME=openhpi\n",
        "%env KAGGLE_KEY=das_ist_der_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc85b549",
      "metadata": {
        "id": "dc85b549"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7acd47fd",
      "metadata": {
        "id": "7acd47fd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "warnings.simplefilter('always', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed6ec2c",
      "metadata": {
        "id": "9ed6ec2c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import rcParams\n",
        "import string\n",
        "rcParams['figure.figsize'] = 14, 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a39fca",
      "metadata": {
        "id": "94a39fca"
      },
      "outputs": [],
      "source": [
        "def show_accuracy_loss_plot(history, num_epochs):\n",
        "    epochs = [i for i in range(num_epochs)]\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = history.history['accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "    fig.set_size_inches(16,9)\n",
        "    \n",
        "    ax[0].plot(epochs , train_acc , 'go-' , label = 'Train Accuracy')\n",
        "    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Test Accuracy')\n",
        "    ax[0].set_title('Training & Test Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "    \n",
        "    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
        "    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')\n",
        "    ax[1].set_title('Training & Test Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5e3bf1",
      "metadata": {
        "id": "ff5e3bf1"
      },
      "source": [
        "# 4.6 Bilderkennung 2\n",
        "\n",
        "<img width=70% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/cover_sign_language.jpg\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd1fcc8",
      "metadata": {
        "id": "dbd1fcc8"
      },
      "source": [
        "# Einlesen der Daten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff92d09",
      "metadata": {
        "id": "fff92d09"
      },
      "outputs": [],
      "source": [
        "!pip3 install kaggle\n",
        "!kaggle datasets download -d datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gLY6KNFTNuTw",
      "metadata": {
        "id": "gLY6KNFTNuTw"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"sign-language-mnist.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62fe6bc6",
      "metadata": {
        "id": "62fe6bc6"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"sign_mnist_train.csv\")\n",
        "df_test = pd.read_csv(\"sign_mnist_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3b29d1",
      "metadata": {
        "id": "5a3b29d1"
      },
      "outputs": [],
      "source": [
        "y_test = df_test[\"label\"]\n",
        "y_train = df_train[\"label\"]\n",
        "del df_train['label']\n",
        "del df_test['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26af01e2",
      "metadata": {
        "id": "26af01e2"
      },
      "outputs": [],
      "source": [
        "# Normalisierung\n",
        "X_train = df_train.values/255\n",
        "X_test = df_test.values/255\n",
        "\n",
        "# Reshaping von 1D zu 3D\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_test = X_test.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f63bd8",
      "metadata": {
        "id": "a6f63bd8"
      },
      "outputs": [],
      "source": [
        "alphabetic_label = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a260d8",
      "metadata": {
        "id": "04a260d8"
      },
      "source": [
        "# Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867f1c16",
      "metadata": {
        "id": "867f1c16"
      },
      "outputs": [],
      "source": [
        "lb=LabelBinarizer()\n",
        "lb.fit(y_train)\n",
        "y_test_oh = lb.transform(y_test)\n",
        "y_train_oh = lb.transform(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f634867",
      "metadata": {
        "id": "2f634867"
      },
      "source": [
        "# Erweiterung des Datensatzes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf7dbaa",
      "metadata": {
        "id": "ccf7dbaa"
      },
      "source": [
        "Ende der vorherigen Einheit haben wir ein \"komplexeres CNN\" mit mehr Layern für unser Problem verwendet. In dieser Einheit wollen wir noch einen weiteren Ansatz testen, um die Ergebnisse unseres Modells zu verbessern. \n",
        "\n",
        "Wir wollen im Folgenden unseren Bilddatensatz erweitern, indem wir kleine Veränderungen an existierenden Bildern vornehmen und diese auch im Training verwenden. Potentiell können wir Bilder verschieben, verzerren, Zoom anwenden oder spiegeln. Auf Spiegelung haben wir in diesem Datensatz bewusst verzichtet. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bdab6e",
      "metadata": {
        "id": "97bdab6e"
      },
      "outputs": [],
      "source": [
        "data_augmentation = ImageDataGenerator(\n",
        "                                  rotation_range = 0,\n",
        "                                  height_shift_range=0.2,\n",
        "                                  width_shift_range=0.2,\n",
        "                                  shear_range=0,\n",
        "                                  horizontal_flip=False,\n",
        "                                  vertical_flip=False)\n",
        "data_augmentation.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9518534",
      "metadata": {
        "id": "c9518534"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(128,kernel_size=(5,5),\n",
        "                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\n",
        "model.add(Conv2D(64,kernel_size=(2,2),\n",
        "                strides=1,activation='relu',padding='same'))\n",
        "model.add(MaxPool2D((2,2),2,padding='same'))\n",
        "model.add(Conv2D(32,kernel_size=(2,2),\n",
        "                strides=1,activation='relu',padding='same'))\n",
        "model.add(MaxPool2D((2,2),2,padding='same'))\n",
        "          \n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=256,activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(units=128,activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(units=24,activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04a946d",
      "metadata": {
        "id": "b04a946d"
      },
      "source": [
        "Wir trainieren hier das gleiche Modell wie vorher auch gezeigt. Nur verwenden wir nicht direkt die Trainingsdaten, sondern erweitern diese künstlich vor dem Training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aab68db",
      "metadata": {
        "id": "6aab68db"
      },
      "outputs": [],
      "source": [
        "history = model.fit(data_augmentation.flow(X_train,y_train_oh,batch_size=128),\n",
        "         epochs = 40,\n",
        "          shuffle=1,\n",
        "           validation_data=(X_test,y_test_oh),\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e582aafb",
      "metadata": {
        "id": "e582aafb"
      },
      "outputs": [],
      "source": [
        "show_accuracy_loss_plot(history, num_epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7775474b",
      "metadata": {
        "id": "7775474b"
      },
      "source": [
        "Wir sehen hier eine Verbesserung auf bis zu 99% Accuracy. Da die Klassen in unserem Datensatz sehr ausgeglichen sind, ist die Accuracy eine valide Metrik für die Analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd07ea27",
      "metadata": {
        "id": "cd07ea27"
      },
      "source": [
        "# Transfer Learning & Pretrained Computer Vision Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee85351",
      "metadata": {
        "id": "dee85351"
      },
      "source": [
        "In dem folgenden Teil dieser Einheit gehen wir auf ein Thema ein, das nicht zwangsläufig für unsere Bilderkennung von Gebärdensprache wichtig ist, dennoch aber sehr spannend ist: Transfer Learning und Pretrained Machine Learning Models.\n",
        "\n",
        "Transferlernen ist ein Teilgebiet des maschinellen Lernens und der künstlichen Intelligenz, das darauf abzielt, das aus einer Aufgabe (Ausgangsaufgabe) gewonnene Wissen auf eine andere, aber ähnliche Aufgabe (Zielaufgabe) anzuwenden.\n",
        "Dabei versucht man den Lernvorgangs in einer neuen Aufgabe durch den Transfer von Wissen zu verbessern und zu verschnellern. Transfer Learning kann die Zeit, neue komplexe Netzwerke zu definieren und zu trainieren deutlich reduzieren. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7027bf9b",
      "metadata": {
        "id": "7027bf9b"
      },
      "source": [
        "Für Transfer Learning werden sogenannte pretrained Modelle verwendet. Diese werden meist von Forschungsgruppen oder Tech-Unternehmen erstellt und auf großen Datensätzen trainiert (z.B. ImageNet oder Wikipedia Corpus). Diese Modelle werden anschließend veröffentlicht und für andere EntwicklerInnen zur Verfügung gestellt. \n",
        "\n",
        "Transfer Learning ist nicht beschränkt auf den Bereich Computer Vision.\n",
        "Hier sind einige Beispiele von pretrained Neural Networks für Computer Vision Aufgaben. Diese können unter anderem für Aufgaben wie Image Classification, Neural Style Transfer oder Anomalie Erkennung genutzt werden. \n",
        "\n",
        "- VGG16\n",
        "- VGG19\n",
        "- Inceptionv3 (GoogLeNet)\n",
        "- ResNet50\n",
        "- EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9796df2e",
      "metadata": {
        "id": "9796df2e"
      },
      "source": [
        "In dieser Einheit werden wir ein VGG16 verwenden mit pretrained Parameters basierend auf Training auf ImageNet, einem großen Datensatz mit über 14 Millionen Bildern. Hierfür müssen wir allerdings unsere Bilder auf eine Größe von 48x48 skalieren, da VGG16 diesen Input als Minimalgröße erwartet. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83da1f4",
      "metadata": {
        "id": "a83da1f4"
      },
      "outputs": [],
      "source": [
        "X_train = df_train.values\n",
        "X_test = df_test.values\n",
        "\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_test = X_test.reshape(-1,28,28,1)\n",
        "\n",
        "x_train_t = np.stack([X_train.reshape(X_train.shape[0],28,28)]*3, axis=3).reshape(X_train.shape[0],28,28,3)\n",
        "x_test_t = np.stack([X_test.reshape(X_test.shape[0],28,28)]*3, axis=3).reshape(X_test.shape[0],28,28,3)\n",
        "x_train_t.shape, x_test_t.shape\n",
        "\n",
        "\n",
        "x_train_tt = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in x_train_t])\n",
        "x_test_tt = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in x_test_t])\n",
        "x_train_tt.shape, x_test_tt.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1134d36d",
      "metadata": {
        "id": "1134d36d"
      },
      "source": [
        "Nun wollen wir das VGG16 Modell einmal laden. Wir verwenden hier die bereits trainierten Parameter auf dem Datensatz \"ImageNet\". Anschließend definieren wir den Rest des Modelles und fügen noch weitere Dense-Layer hinzu vor dem Output-Layer. \n",
        "\n",
        "Hier der Grobaufbau des VGG16 Modells:\n",
        "\n",
        "<img width=80% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/VGG_2.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b4852a",
      "metadata": {
        "id": "69b4852a"
      },
      "outputs": [],
      "source": [
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(48, 48, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "train_ds = preprocess_input(x_train_tt) \n",
        "test_ds = preprocess_input(x_test_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a987c0a9",
      "metadata": {
        "id": "a987c0a9"
      },
      "outputs": [],
      "source": [
        "flatten_layer = layers.Flatten()\n",
        "dense_layer_1 = layers.Dense(50, activation='relu')\n",
        "dense_layer_2 = layers.Dense(36, activation='relu')\n",
        "prediction_layer = layers.Dense(24, activation='softmax')\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    flatten_layer,\n",
        "    dense_layer_1,\n",
        "    dense_layer_2,\n",
        "    prediction_layer\n",
        "])\n",
        "model.compile(loss = keras.losses.categorical_crossentropy, optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2f5f60",
      "metadata": {
        "id": "9d2f5f60"
      },
      "source": [
        "Wir wollen uns erneut einmal die Zusammenfassung unseres Modells ausgeben lassen. Hierbei sehen wir, dass das Modell mehr als 14 Millionen Parameter hat. Unser vorheriges Modell hat lediglich 200k Parameter. Das Modell ist durchaus deutlich zu groß für unsere aktuelle Aufgabe. Wie oben allerdings betont, geht es bei diesem Teil der Einheit nicht darum, ein besseres Ergebnis zu erhalten, sondern das Konzept von Transfer Learning zu verdeutlichen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15edf890",
      "metadata": {
        "id": "15edf890"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7515fcc9",
      "metadata": {
        "id": "7515fcc9"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds,y_train_oh, batch_size=128,\n",
        "         epochs = 10,\n",
        "          shuffle=1,\n",
        "           validation_data=(test_ds,y_test_oh),\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a6d0c9",
      "metadata": {
        "id": "01a6d0c9"
      },
      "outputs": [],
      "source": [
        "show_accuracy_loss_plot(history, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccb0ee0",
      "metadata": {
        "id": "6ccb0ee0"
      },
      "source": [
        "Das ist das Ende der Einheit zum Thema Erkennung von Gebärdensprache in Bildern. In der folgenden Einheit **4.7 Ergebnis und Auswertung** werden wir uns erneut unser bestes Modell im Detail ansehen und auswerten. Das Transfer Learning Modell werden wir in Einheit 4.7 nicht betrachten.  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "4.6 Bilderkennung 2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
