{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianwarmuth/openhpi-kipraxis/blob/main/Woche%204/4_3_Erster_Blick_in_die_Daten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb899884",
      "metadata": {
        "id": "eb899884"
      },
      "source": [
        "## 0. Installieren aller Pakete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUdfOErezOpd",
      "metadata": {
        "id": "JUdfOErezOpd"
      },
      "outputs": [],
      "source": [
        "# Hier die Kaggle Credentials einfügen (ohne Anführungszeichen)\n",
        "\n",
        "%env KAGGLE_USERNAME=openhpi\n",
        "%env KAGGLE_KEY=das_ist_der_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc85b549",
      "metadata": {
        "id": "dc85b549"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7acd47fd",
      "metadata": {
        "id": "7acd47fd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "warnings.simplefilter('always', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed6ec2c",
      "metadata": {
        "id": "9ed6ec2c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import rcParams\n",
        "import string\n",
        "rcParams['figure.figsize'] = 14, 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a39fca",
      "metadata": {
        "id": "94a39fca"
      },
      "outputs": [],
      "source": [
        "def show_example_pictures(X_train, y_train, alphabetic_label):\n",
        "    p = 0\n",
        "    for i in range(10):\n",
        "        plt.subplot(3,5,p+1)\n",
        "        plt.imshow(X_train[i].reshape(28,28), cmap=\"gray\", interpolation='none')\n",
        "        plt.title(\"Label: {}\".format(alphabetic_label[y_train[i]]))\n",
        "        plt.tight_layout()\n",
        "        p += 1\n",
        "        \n",
        "def show_label_frequency(df_train):\n",
        "    plt.figure(figsize = (20,10)) # Label Count\n",
        "    sns.set(font_scale=3)\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    sns.countplot(df_train['label'], label='medium')\n",
        "    plt.title(\"Frequency of each label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5e3bf1",
      "metadata": {
        "id": "ff5e3bf1"
      },
      "source": [
        "# 4.3 Erster Blick in die Daten\n",
        "\n",
        "<img width=70% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/cover_sign_language.jpg\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72045723",
      "metadata": {
        "id": "72045723"
      },
      "source": [
        "Datensatz: \n",
        "\n",
        "### Sign Language MNIST Dataset\n",
        "Über 34.000 Bilder zu Gebärdensprache ASL (American Sign Language) und dazugehörige Übersetzung der Gesten in das Alphabet in A bis Z. Bilder in 28x28 Pixeln in Graustufen. \n",
        "\n",
        "Quelle: kaggle.com"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb217d9",
      "metadata": {
        "id": "1eb217d9"
      },
      "source": [
        "## Download Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fc0b24",
      "metadata": {
        "id": "00fc0b24"
      },
      "source": [
        "### Manuell\n",
        "via https://www.kaggle.com/datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f172699",
      "metadata": {
        "id": "4f172699"
      },
      "source": [
        "### Via API\n",
        "\n",
        "Hinzufügen der kaggle.json\n",
        "Speichern als ~/.kaggle/kaggle.json auf Linux, OSX, oder anderen UNIX-basierten Betriebssystemen und unter C:\\Users<Windows-username>.kaggle\\kaggle.json auf Windows\n",
        "\n",
        "Siehe https://www.kaggle.com/docs/api oder https://github.com/Kaggle/kaggle-api\n",
        "        \n",
        "Beispiel:\n",
        "~/kaggle/kaggle.json\n",
        "\n",
        "{\"username\":\"openHPI\",\"key\":\"das_ist_der_key\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bef49c",
      "metadata": {
        "id": "c1bef49c"
      },
      "outputs": [],
      "source": [
        "!pip3 install kaggle\n",
        "!kaggle datasets download -d datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k6J3AFYMzYlU",
      "metadata": {
        "id": "k6J3AFYMzYlU"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"sign-language-mnist.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a160edae",
      "metadata": {
        "id": "a160edae"
      },
      "source": [
        "## Übersicht über alle Dateien\n",
        "\n",
        "Zunächst sehen wir uns alle Dateien in diesem Datensatz einmal an. In dieser Woche wollen wir aus Bildern Gebärdensprache erkennen und diese in das Alphabet von A bis Z übersetzen. Dafür wechseln wir zunächst in das Verzeichnis mit den Daten und lassen uns alle Dateien auflisten. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c30b648",
      "metadata": {
        "id": "2c30b648"
      },
      "outputs": [],
      "source": [
        "!ls -gnG\n",
        "!cd sign_mnist_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb7db1e",
      "metadata": {
        "id": "2bb7db1e"
      },
      "source": [
        "# Grundlagen Gebärdensprachen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63369d",
      "metadata": {
        "id": "0b63369d"
      },
      "source": [
        "Die Amerikanische Gebärdensprache (American Sign Language, kurz ASL) ist eine vollständige, natürliche Sprache, die dieselben linguistischen Eigenschaften wie gesprochene Sprachen aufweist, deren Grammatik sich jedoch vom Englischen unterscheidet. ASL wird durch Bewegungen der Hände und des Gesichts ausgedrückt. Sie ist die Hauptsprache vieler Nordamerikaner, die taub oder schwerhörig sind und wird auch von vielen hörenden Menschen verwendet. \n",
        "\n",
        "Die American Sign Language unterscheidet sich durchaus deutlich von der deutschen Gebärdensprache. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454dfbfd",
      "metadata": {
        "id": "454dfbfd"
      },
      "source": [
        "<img width=70% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/sign.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f831a8",
      "metadata": {
        "id": "92f831a8"
      },
      "source": [
        "# Datensatz Sign Languag MNIST\n",
        "\n",
        "Als ersten Schritt werden wir uns zunächst eine der beiden Dateien mit Bilddaten etwas genauer ansehen. Jeweils **sign_mnist_train.csv** und **sign_mnist_test.csv** enthalten die Bilddaten. Hier wurde die Teilung in Train- und Test-Daten, die wir in einer der vorherigen Einheiten bereits angesprochen haben, bereits vorgenommen.\n",
        "\n",
        "Wenn wir uns allerdings die Dateien einmal als DataFrame einlesen, so fällt auf, dass die einzelnen Pixel jeweils in unterschiedlichen Spalten gespeichert werden und nicht jedes Bild in einer eigenen Datei abgelegt ist. So ist also eine Reihe ein Bild mit insgesamt 284 Pixel - das bildest zusammen ein 28x28 Pixel Bild. Zudem sind die Bilder nur in Graustufen verfügbar. Im Vergleich zur Auflösung in modernen Bildschirmen mit mehreren Tausend Pixeln auf beiden Achsen ist das eine sehr geringe Auflösung. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd8e57d",
      "metadata": {
        "id": "bfd8e57d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"sign_mnist_train.csv\")\n",
        "df_test = pd.read_csv(\"sign_mnist_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dd38b7",
      "metadata": {
        "id": "17dd38b7"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3e2a52",
      "metadata": {
        "id": "1e3e2a52"
      },
      "source": [
        "# Ein Beispielbild"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840f5425",
      "metadata": {
        "id": "840f5425"
      },
      "source": [
        "Im Folgenden sehen wir uns ein Beispielbild aus dem Datensatz einmal genauer an. Vorher müssen wir jedoch die einzelnen Pixel Werte wieder in die passende Form bringen. \n",
        "\n",
        "Die erste Spalte beinhaltet die Labels für die einzelnen Bilder. Die Bilddaten speichern wir jeweils in **X_train** und **X_test**, jedoch ohne das Label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4cfa10f",
      "metadata": {
        "id": "d4cfa10f"
      },
      "outputs": [],
      "source": [
        "X_train = df_train.drop([\"label\"], axis=1).values\n",
        "X_test = df_test.drop([\"label\"], axis=1).values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c382b4eb",
      "metadata": {
        "id": "c382b4eb"
      },
      "source": [
        "Die Label speichern wir in den Variablen **y_train** und **y_test**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62f770a",
      "metadata": {
        "id": "e62f770a"
      },
      "outputs": [],
      "source": [
        "y_test = df_test[\"label\"]\n",
        "y_train = df_train[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943bfa69",
      "metadata": {
        "id": "943bfa69"
      },
      "source": [
        "Wie wir in der Theorie-Einheit gelernt haben, iterieren Convolutional Neural Networks (CNNs) über 2D Matrizen (3D wenn man die einzelnen Channels betrachtet). Daher müssen wir unsere Bilder auch in dieses Format bringen. Hier auch in 3 Dimensionen, wobei die dritte Dimension nur einen Wert für die Graustufe von 0 bis 255 enthält. \n",
        "\n",
        "<img width=40% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/cnn.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdcfd5a",
      "metadata": {
        "id": "7fdcfd5a"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_test = X_test.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491f4bc4",
      "metadata": {
        "id": "491f4bc4"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ff2037",
      "metadata": {
        "id": "61ff2037"
      },
      "source": [
        "Die Labels in unserem Datensatz sind aktuell nach wie vor noch nur Zahlen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0764ce",
      "metadata": {
        "id": "da0764ce"
      },
      "outputs": [],
      "source": [
        "np.sort(df_train[\"label\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33250c06",
      "metadata": {
        "id": "33250c06"
      },
      "source": [
        "Da wir auch überprüfen wollen, ob die Bilder in die richtigen Buchstaben aus dem Alphabet übersetzt werden, erstellen wir eine Liste um die Indices jeweils in die Buchstaben zu verwandeln. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ededbe",
      "metadata": {
        "id": "78ededbe"
      },
      "outputs": [],
      "source": [
        "alphabetic_label = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c039bf4e",
      "metadata": {
        "id": "c039bf4e"
      },
      "source": [
        "Im Folgenden werden wir uns einmal einige Beispielbilder ansehen aus dem Datensatz. Hier sieht man noch einmal, dass die Bilder eine relativ geringe Auflösung haben.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e62310e",
      "metadata": {
        "id": "0e62310e"
      },
      "outputs": [],
      "source": [
        "show_example_pictures(X_train, y_train, alphabetic_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a4bf52",
      "metadata": {
        "id": "72a4bf52"
      },
      "source": [
        "Insgesamt hat auch das ASL-Alphabet 26 verschiedene Buchstaben. Geben wir uns allerdings einmal die Anzahl der Labels aus. Hierbei fällt auf, dass wir nur 24 Labels haben. Das liegt daran, dass der Buchstabe **J** und **Z** mit einer Bewegung dargestellt werden. Daher sind diese nicht Teil des Datensatzes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4473b02e",
      "metadata": {
        "id": "4473b02e"
      },
      "outputs": [],
      "source": [
        "df_train[\"label\"].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "016e5ba4",
      "metadata": {
        "id": "016e5ba4"
      },
      "source": [
        "Als letzten Schritt dieser Einheit sehen wir uns einmal die Verteilung der verschiedenen Buchstaben im Datensatz an. Hierfür erstellen wir einen sogenannten Barplot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de02063f",
      "metadata": {
        "id": "de02063f"
      },
      "outputs": [],
      "source": [
        "show_label_frequency(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ab20e6",
      "metadata": {
        "id": "e0ab20e6"
      },
      "source": [
        "Hier sehen wir, dass für jeden Buchstaben zwischen circa 1.200 und 1.000 Bilder im Datensatz vorhanden sind. Die verschiedenen Klassen, in die wir später die Bilder klassifizieren wollen, sind also relativ gleich verteilt. Hier fällt auch noch einmal auf, dass Label 9 und 25 fehlen (die Buchstaben **J** und **Z**). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dcb991",
      "metadata": {
        "id": "f6dcb991"
      },
      "source": [
        "Das war es auch schon zur Einheit **Erster Blick in die Daten**. In der nächsten Einheit werden wir versuchen mit einem Convolutional Neural Network die einzelnen Bilder richtig zu klassifizieren, also jeweils einen Buchstaben zuordnen. "
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "4.3 Erster Blick in die Daten.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
