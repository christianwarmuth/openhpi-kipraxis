{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianwarmuth/openhpi-kipraxis/blob/main/Woche%204/4_5_Bilderkennung_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb899884",
      "metadata": {
        "id": "eb899884"
      },
      "source": [
        "## 0. Installieren aller Pakete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMsrxDgt1kWn",
      "metadata": {
        "id": "jMsrxDgt1kWn"
      },
      "outputs": [],
      "source": [
        "# Hier die Kaggle Credentials einfügen (ohne Anführungszeichen)\n",
        "\n",
        "%env KAGGLE_USERNAME=openhpi\n",
        "%env KAGGLE_KEY=das_ist_der_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc85b549",
      "metadata": {
        "id": "dc85b549"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7acd47fd",
      "metadata": {
        "id": "7acd47fd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "warnings.simplefilter('always', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed6ec2c",
      "metadata": {
        "id": "9ed6ec2c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import rcParams\n",
        "import string\n",
        "rcParams['figure.figsize'] = 14, 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a39fca",
      "metadata": {
        "id": "94a39fca"
      },
      "outputs": [],
      "source": [
        "def show_accuracy_loss_plot(history, num_epochs):\n",
        "    epochs = [i for i in range(num_epochs)]\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = history.history['accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "    fig.set_size_inches(16,9)\n",
        "    \n",
        "    ax[0].plot(epochs , train_acc , 'go-' , label = 'Train Accuracy')\n",
        "    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Test Accuracy')\n",
        "    ax[0].set_title('Training & Test Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "    \n",
        "    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
        "    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')\n",
        "    ax[1].set_title('Training & Test Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5e3bf1",
      "metadata": {
        "id": "ff5e3bf1"
      },
      "source": [
        "# 4.5 Bilderkennung 1 \n",
        "<img width=70% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/cover_sign_language.jpg\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72045723",
      "metadata": {
        "id": "72045723"
      },
      "source": [
        "Datensatz: \n",
        "\n",
        "### Sign Language MNIST Dataset\n",
        "Über 34.000 Bilder zu Gebärdensprache und dazugehörige Übersetzung der Gesten in das Alphabet in A bis Z. Bilder in 28x28 Pixeln in Graustufen. \n",
        "\n",
        "Quelle: kaggle.com"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb217d9",
      "metadata": {
        "id": "1eb217d9"
      },
      "source": [
        "## Download Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fc0b24",
      "metadata": {
        "id": "00fc0b24"
      },
      "source": [
        "### Manuell\n",
        "via https://www.kaggle.com/datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f172699",
      "metadata": {
        "id": "4f172699"
      },
      "source": [
        "### Via API\n",
        "\n",
        "Hinzufügen der kaggle.json\n",
        "Speichern als ~/.kaggle/kaggle.json auf Linux, OSX, oder anderen UNIX-basierten Betriebssystemen und unter C:\\Users<Windows-username>.kaggle\\kaggle.json auf Windows\n",
        "\n",
        "Siehe https://www.kaggle.com/docs/api oder https://github.com/Kaggle/kaggle-api\n",
        "        \n",
        "Beispiel:\n",
        "~/kaggle/kaggle.json\n",
        "\n",
        "{\"username\":\"openHPI\",\"key\":\"das_ist_der_key\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bef49c",
      "metadata": {
        "id": "c1bef49c"
      },
      "outputs": [],
      "source": [
        "!pip3 install kaggle\n",
        "!kaggle datasets download -d datamunge/sign-language-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k0BYyrXm1wyi",
      "metadata": {
        "id": "k0BYyrXm1wyi"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"sign-language-mnist.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a160edae",
      "metadata": {
        "id": "a160edae"
      },
      "source": [
        "## Was wir erreichen wollen\n",
        "\n",
        "Ziel dieser Einheit ist es, Bilder von amerikanischer Gebärdensprache in das Alphabet von A bis Z zu übersetzen. Hierbei haben wir einen Trainings-Datensatz mit Labels für die verschiedenen Bilder. \n",
        "\n",
        "<img width=70% src=\"https://raw.githubusercontent.com/christianwarmuth/openhpi-kipraxis/main/images/sign_color.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e835a02",
      "metadata": {
        "id": "5e835a02"
      },
      "source": [
        "# Einlesen der Daten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd8e57d",
      "metadata": {
        "id": "bfd8e57d"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"sign_mnist_train.csv\")\n",
        "df_test = pd.read_csv(\"sign_mnist_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a95cbe1",
      "metadata": {
        "id": "7a95cbe1"
      },
      "source": [
        "Nachdem wir die Dateien eingelesen haben, speichern wir jeweils die Labels in eigenen Variablen und entfernen sie vom ursprünglichen Datensatz. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da5077c",
      "metadata": {
        "id": "5da5077c"
      },
      "outputs": [],
      "source": [
        "y_test = df_test[\"label\"]\n",
        "y_train = df_train[\"label\"]\n",
        "del df_train['label']\n",
        "del df_test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c677be16",
      "metadata": {
        "id": "c677be16"
      },
      "source": [
        "Wir normalisieren zunächst alle Pixel-Werte für die Bilder und konvertieren die einzelnen Pixel-Reihen zu 3D Bildern. Die dritte Dimension hier ist der Farbwert (hier nur eine Graustufe)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dd38b7",
      "metadata": {
        "id": "17dd38b7"
      },
      "outputs": [],
      "source": [
        "# Normalisierung\n",
        "X_train = df_train.values/255\n",
        "X_test = df_test.values/255\n",
        "\n",
        "# Reshaping von 1D zu 3D\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_test = X_test.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08500545",
      "metadata": {
        "id": "08500545"
      },
      "source": [
        "Anschließend erstellen wir erneut eine Liste mit den verschiedenen Buchstaben, in die wir die einzelnen Bilder übersetzen wollen. Im Datensatz sind nur Nummern gegeben, aber mit dieser Liste können wir die jeweilige Nummer zu ihrem Buchstaben übersetzen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ededbe",
      "metadata": {
        "id": "78ededbe"
      },
      "outputs": [],
      "source": [
        "alphabetic_label = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ab20e6",
      "metadata": {
        "id": "e0ab20e6"
      },
      "source": [
        "## Label Encoding\n",
        "\n",
        "Die Labels wollen wir allerdings noch mit einem One-Hot-Encoder verarbeiten. Für jedes Label wird eine eigene Spalte eingeführt und nur eine \"1\" eingefügt, wenn ein Bild diesem Label angehört. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e78a09d",
      "metadata": {
        "id": "9e78a09d"
      },
      "outputs": [],
      "source": [
        "lb=LabelBinarizer()\n",
        "lb.fit(y_train)\n",
        "y_test_oh = lb.transform(y_test)\n",
        "y_train_oh = lb.transform(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b88afbd",
      "metadata": {
        "id": "4b88afbd"
      },
      "outputs": [],
      "source": [
        "# Ursprüngliches Label\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15edf890",
      "metadata": {
        "id": "15edf890"
      },
      "outputs": [],
      "source": [
        "# Label nach One-Hot-Encoding\n",
        "y_train_oh[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3914c52",
      "metadata": {
        "id": "e3914c52"
      },
      "source": [
        "# Einfaches Modell\n",
        "\n",
        "Wir definieren zunächst ein einfaches Convolutional Neural Network mit einem Convoluational Layer, einem Pooling Layer und einem Dense Layer (Fully-Connected Layer) vor dem Output-Layer. Damit wir etwas genauer verstehen, wie dieses Modell aufgebaut ist, so geben wir uns anschließend die Form des Modelles einmal graphisch aus. \n",
        "\n",
        "Wir sehen hier den Aufbau des Modells sowie die Anzahl der Parameter, die trainiert werden. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347182d8",
      "metadata": {
        "id": "347182d8"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(128,kernel_size=(5,5),\n",
        "                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))         \n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=256,activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(units=24,activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f286bd",
      "metadata": {
        "id": "88f286bd"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a7d9eb",
      "metadata": {
        "id": "18a7d9eb"
      },
      "source": [
        "Zwar haben wir jetzt das Modell definiert, allerdings haben wir noch nichts trainiert. Das wollen wir im nächsten Schritt machen. Wir trainieren für 20 Epochen (also 20 Läufe durch den Datensatz). Im Laufe des Trainings speichern wir uns jeweils die Performance auf den Trainings- und Testdaten, um diese anschließend in einem Chart ausgeben zu können. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02fd667",
      "metadata": {
        "id": "a02fd667"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_train_oh,\n",
        "         epochs = 20,\n",
        "          shuffle=1,\n",
        "           validation_data=(X_test,y_test_oh),\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d943f13",
      "metadata": {
        "id": "6d943f13"
      },
      "source": [
        "Wir wollen uns nun einmal unser Training über die Zeit ansehen. Wie wir sehen, erreichen wir eine Accuracy von circa 92%. Optimal ist das noch nicht, aber durchaus ein sehr guter Anfang. Im Folgenden wollen wir noch einmal sehen, ob wir mit einem komplexeren Modell bessere Ergebnisse erzielen.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb1edf2",
      "metadata": {
        "id": "6bb1edf2"
      },
      "outputs": [],
      "source": [
        "show_accuracy_loss_plot(history, num_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12d74e2",
      "metadata": {
        "id": "d12d74e2"
      },
      "source": [
        "# Komplexeres Modell\n",
        "\n",
        "Nun versuchen wir eine komplexere Modell-Architektur. Wir fügen noch jeweils zwei Convolutional Layer und Pooling Layer hinzu. Wir werden auch noch einen weiteren Dense-Layer hinzufügen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0143e1",
      "metadata": {
        "id": "7c0143e1"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(128,kernel_size=(5,5),\n",
        "                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\n",
        "model.add(Conv2D(64,kernel_size=(2,2),\n",
        "                strides=1,activation='relu',padding='same'))\n",
        "model.add(MaxPool2D((2,2),2,padding='same'))\n",
        "model.add(Conv2D(32,kernel_size=(2,2),\n",
        "                strides=1,activation='relu',padding='same'))\n",
        "model.add(MaxPool2D((2,2),2,padding='same'))\n",
        "          \n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=256,activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(units=128,activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(units=24,activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807c604e",
      "metadata": {
        "id": "807c604e"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941f8ca4",
      "metadata": {
        "id": "941f8ca4"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_train_oh,\n",
        "         epochs = 20,\n",
        "          shuffle=1,\n",
        "           validation_data=(X_test,y_test_oh),\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f09530a",
      "metadata": {
        "id": "9f09530a"
      },
      "outputs": [],
      "source": [
        "show_accuracy_loss_plot(history, num_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56367ebb",
      "metadata": {
        "id": "56367ebb"
      },
      "source": [
        "Im Vergleich zum einfachen Modell haben wir nur eine minimale Verbesserung gesehen auf circa 94%. In der nächsten Einheit werden wir noch eine Strategie anwenden, um bessere Klassifikationsergebnisse mit unserem Convolutional Neural Network zu erreichen. "
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "name": "4.5 Bilderkennung 1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
